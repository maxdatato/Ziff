{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@jamielewislewis i cant believe it, it really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>having a vodka tonic and looking forward to go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>@ddlovatofans1neg1 Could you follow me please....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>@jordanknight for once.................. PLEAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>Had a dream about a walk in fast food resturau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0       neg  @jamielewislewis i cant believe it, it really ...\n",
       "1       pos  having a vodka tonic and looking forward to go...\n",
       "2       pos  @ddlovatofans1neg1 Could you follow me please....\n",
       "3       pos  @jordanknight for once.................. PLEAS...\n",
       "4       neg  Had a dream about a walk in fast food resturau..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Importing the dataset\n",
    "df = pd.read_csv('sentiment.tsv', delimiter = '\\t', header=None,names = [\"sentiment\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python file uses the following encoding: utf-8\n",
    "import re\n",
    "\n",
    "# Hashtags\n",
    "hash_regex = re.compile(r\"#(\\w+)\")\n",
    "def hash_repl(match):\n",
    "    return '__HASH_'+match.group(1).upper()    \n",
    "\n",
    "# Handels\n",
    "hndl_regex = re.compile(r\"@(\\w+)\")\n",
    "def hndl_repl(match):\n",
    "    return '__HNDL'#_'+match.group(1).upper()    \n",
    "\n",
    "# URLs\n",
    "url_regex = re.compile(r\"(http|https|ftp)://[a-zA-Z0-9\\./]+\")\n",
    "\n",
    "# Spliting by word boundaries\n",
    "word_bound_regex = re.compile(r\"\\W+\")\n",
    "\n",
    "# Repeating words like hurrrryyyyyy\n",
    "rpt_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE);\n",
    "def rpt_repl(match):\n",
    "    return match.group(1)+match.group(1)\n",
    "\n",
    "# Emoticons\n",
    "emoticons = \\\n",
    "    [('__EMOT_SMILEY',\t[':-)', ':)', '(:', '(-:', ] )\t,\\\n",
    "        ('__EMOT_LAUGH',\t\t[':-D', ':D', 'X-D', 'XD', 'xD', ] )\t,\\\n",
    "        ('__EMOT_LOVE',\t\t['<3', ':\\*', ] )\t,\\\n",
    "        ('__EMOT_WINK',\t\t[';-)', ';)', ';-D', ';D', '(;', '(-;', ] )\t,\\\n",
    "        ('__EMOT_FROWN',\t\t[':-(', ':(', '(:', '(-:', ] )\t,\\\n",
    "        ('__EMOT_CRY',\t\t[':,(', ':\\'(', ':\"(', ':(('] )\t,\\\n",
    "    ]\n",
    "\n",
    "#For emoticon regexes\n",
    "def escape_paren(arr):\n",
    "    return [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
    "def regex_union(arr):\n",
    "    return '(' + '|'.join( arr ) + ')'\n",
    "emoticons_regex = [ (repl, re.compile(regex_union(escape_paren(regx))) ) \\\n",
    "                    for (repl, regx) in emoticons ]\n",
    "\n",
    "def processAll(text):\n",
    "\n",
    "    text = re.sub( hash_regex, hash_repl, text )\n",
    "    text = re.sub( hndl_regex, hndl_repl, text )\n",
    "    text = re.sub( url_regex, ' __URL ', text )\n",
    "\n",
    "    for (repl, regx) in emoticons_regex :\n",
    "        text = re.sub(regx, ' '+repl+' ', text)\n",
    "\n",
    "    text = text.replace('\\'','')\n",
    "    text = re.sub( word_bound_regex ,' ', text )\n",
    "    text = re.sub( rpt_regex, rpt_repl, text )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /opt/pynb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /opt/pynb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, len(df)):\n",
    "#     tokens = re.sub('[^a-zA-Z]', ' ', df['text'][i])\n",
    "    tokens = df['text'][i]\n",
    "    tokens = tokens.lower()\n",
    "    tokens = processAll(tokens)\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    tokens = tokenizer.tokenize(tokens)\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word) >= 3]    \n",
    "    tokens = [word for word in tokens if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "#   stemmer = PorterStemmer()\n",
    "#   stemmer = LancasterStemmer()\n",
    "#   tokens = [stemmer.stem(word) for word in tokens]    \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "    \n",
    "#     replacer = SpellingReplacer()\n",
    "#     tokens = [replacer.replace(word) for word in tokens]\n",
    "        \n",
    "    tokens = ' '.join(tokens)\n",
    "    corpus.append(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3),token_pattern=r'\\b\\w+\\b', min_df=5,max_features = 1500)\n",
    "X = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer(max_features = 1500)\n",
    "# X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# features = vectorizer.fit_transform(corpus)\n",
    "# X = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1000\n",
    "j = 10\n",
    "vectorizer.get_feature_names( )[i:i+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1neg', '__hndl aww', '__hndl hey', '__hndl sorry', '__hndl wow', 'age', 'always', 'asleep', 'back work', 'best', 'bored', 'btw', 'cant wait', 'chill', 'coming', 'crazy', 'day today', 'dont', 'early', 'ever', 'fan', 'final', 'follow', 'free', 'get back', 'going bed', 'graduation', 'hanging', 'hear', 'homework', 'husband', 'ive', 'last', 'let', 'local', 'love', 'man', 'minute', 'mother', 'name', 'okay', 'party', 'play', 'project', 'ready', 'room', 'second', 'shopping', 'sister', 'soon', 'starting', 'summer', 'talking', 'theyre', 'time', 'trip', 'ugh', 'wake', 'way', 'win', 'world', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names( )\n",
    "print(feature_names[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prop.table(table(df$sentiment))\n",
    "yes_no_cols = [\"sentiment\"]\n",
    "df[yes_no_cols] = df[yes_no_cols] == 'pos'\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy: 0.755 (0.024)\n",
      "SGDClassifier Accuracy: 0.727 (0.032)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn import linear_model\n",
    "# from sklearn import tree\n",
    "# from sklearn import svm\n",
    "# from sklearn import ensemble\n",
    "# from sklearn import neighbors\n",
    "\n",
    "kf = KFold(len(y),n_folds=10,shuffle=True)\n",
    "kf2 = StratifiedKFold(y,n_folds=10,shuffle=True)\n",
    "\n",
    "\n",
    "results = cross_val_score(LogisticRegression(), X = X, y = y, scoring = \"roc_auc\", cv = kf)\n",
    "print(\"LogisticRegression Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "results = cross_val_score(SGDClassifier(loss='log', penalty='l1'), X = X, y = y, scoring = \"roc_auc\", cv = kf)\n",
    "print(\"SGDClassifier Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "# results = cross_val_score(GaussianNB(), X = X, y = y, scoring = \"roc_auc\", cv = kf)\n",
    "# print(\"GaussianNB Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "# results = cross_val_score(ensemble.RandomForestClassifier(), X = X, y = y, scoring = \"roc_auc\", cv = kf)\n",
    "# print(\"RandomForest Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "# results = cross_val_score( ensemble.GradientBoostingClassifier(), X = X, y = y,scoring = \"roc_auc\", cv = kf)\n",
    "# print(\"GradientBoosting Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score:  0.751816642371\n",
      "Best parameters:  {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, scoring = \"roc_auc\", cv=10)\n",
    "grid.fit(X, y)\n",
    "print(\"Best cross-validation score: \", grid.best_score_)\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score:  0.748532102467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': 1, 'tfidfvectorizer__ngram_range': (1, 3)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5)]}\n",
    "grid = GridSearchCV(pipe, param_grid, scoring = \"roc_auc\", cv=10)\n",
    "grid.fit(corpus, y)\n",
    "print(\"Best cross-validation score: \", grid.best_score_)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "def DefineModel1():    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=615))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "def DefineModel2():    \n",
    "    \n",
    "    embed_dim = 128\n",
    "    lstm_out = 196\n",
    "    max_fatures = 1500\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))    \n",
    "    model.add(LSTM(lstm_out))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy']) \n",
    "    return model  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "55s - loss: 0.6959 - acc: 0.5022\n",
      "Epoch 2/50\n",
      "54s - loss: 0.6934 - acc: 0.5017\n",
      "Epoch 3/50\n",
      "54s - loss: 0.6934 - acc: 0.5077\n",
      "Epoch 4/50\n",
      "54s - loss: 0.6934 - acc: 0.5007\n",
      "Epoch 5/50\n",
      "54s - loss: 0.6930 - acc: 0.4888\n",
      "Epoch 6/50\n",
      "54s - loss: 0.6926 - acc: 0.5077\n",
      "Epoch 7/50\n",
      "54s - loss: 0.6931 - acc: 0.4953\n",
      "Epoch 8/50\n",
      "54s - loss: 0.6922 - acc: 0.4903\n",
      "Epoch 9/50\n",
      "54s - loss: 0.6944 - acc: 0.5062\n",
      "Epoch 10/50\n"
     ]
    }
   ],
   "source": [
    "model = DefineModel2()     \n",
    "model.fit(X, y, epochs=50, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=DefineModel2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# evaluate using 10-fold cross validation\n",
    "# kf = KFold(len(y),n_folds=10,shuffle=True)\n",
    "kf2 = StratifiedKFold(y,n_folds=10,shuffle=True, random_state=7)\n",
    "results = cross_val_score(model, X, y,scoring = \"roc_auc\", cv=kf2)\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of roc_auc:\n",
    "KerasClassifier: Accuracy: 0.713 (0.038)\n",
    "LogisticRegression Accuracy: 0.751 (0.017)\n",
    "SGDClassifier Accuracy: 0.723 (0.018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
