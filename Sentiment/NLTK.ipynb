{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /opt/pynb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /opt/pynb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stopset = set(stopwords.words('english')) - set(('over', 'under', 'below', 'more', 'most', 'no', 'not', 'only', 'such', 'few', 'so', 'too', 'very', 'just', 'any', 'once'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')    \n",
    "ids = negids + posids\n",
    "\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    " \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "trainfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def evaluate_classifier(featx):\n",
    "   \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    " \n",
    "    print ('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "#     print ('pos precision:', nltk.metrics.precision(refsets['pos'], testsets['pos']))\n",
    "#     print ('pos recall:', nltk.metrics.recall(refsets['pos'], testsets['pos']))\n",
    "#     print ('neg precision:', nltk.metrics.precision(refsets['neg'], testsets['neg']))\n",
    "#     print ('neg recall:', nltk.metrics.recall(refsets['neg'], testsets['neg']))\n",
    "    classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy:', 0.728)\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate_classifier(word_feats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
