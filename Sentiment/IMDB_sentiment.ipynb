{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn import naive_bayes, svm, preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection.univariate_selection import chi2, SelectKBest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count to be included for training                      \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# training_model = {\"RF\", \"NB\", \"SVM\", \"BT\", \"no\"}\n",
    "training_model = \"NB\"\n",
    "\n",
    "# feature scaling = {\"standard\", \"signed\", \"unsigned\", \"no\"}\n",
    "# Note: Scaling is needed for SVM\n",
    "scaling = \"no\"\n",
    "\n",
    "# dimension reduction = {\"SVD\", \"chi2\", \"no\"}\n",
    "# Note: For NB models, we cannot perform truncated SVD as it will make input negative\n",
    "# chi2 is the feature selectioin based on chi2 independence test\n",
    "dim_reduce = \"chi2\"\n",
    "num_dim = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@jamielewislewis i cant believe it, it really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>having a vodka tonic and looking forward to go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>@ddlovatofans1neg1 Could you follow me please....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>@jordanknight for once.................. PLEAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>Had a dream about a walk in fast food resturau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0       neg  @jamielewislewis i cant believe it, it really ...\n",
       "1       pos  having a vodka tonic and looking forward to go...\n",
       "2       pos  @ddlovatofans1neg1 Could you follow me please....\n",
       "3       pos  @jordanknight for once.................. PLEAS...\n",
       "4       neg  Had a dream about a walk in fast food resturau..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "df = pd.read_csv('sentiment.tsv', delimiter = '\\t', header=None,names = [\"sentiment\", \"text\"])\n",
    "# dataset = pd.read_csv('sentiment.tsv')\n",
    "df.head()\n",
    "# str(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, len(df)):\n",
    "    tokens = re.sub('[^a-zA-Z]', ' ', df['text'][i])\n",
    "    tokens = tokens.lower()\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    tokens = tokenizer.tokenize(tokens)\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word) >= 3]    \n",
    "    tokens = [word for word in tokens if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "#   stemmer = PorterStemmer()\n",
    "    stemmer = LancasterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "    \n",
    "#     replacer = SpellingReplacer()\n",
    "#     tokens = [replacer.replace(word) for word in tokens]\n",
    "    \n",
    "#     review = stopword_filtered_word_feats(review)\n",
    "#     review = bigram_word_feats(review) \n",
    "#     review = bigrams(review)\n",
    "    \n",
    "    tokens = ' '.join(tokens)\n",
    "    corpus.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
